"""
Unlimited Stock Data Extractor - Completely Free with No Rate Limits
Replaces yfinance-based system with unlimited web scraping and free APIs
Handles 6000+ stocks without any rate limiting issues

This module provides backward-compatible interfaces while using the new unlimited system.
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Union
import os
from dotenv import load_dotenv
import warnings
import json

# Import our unlimited extraction system
from .unlimited_extractor_with_fallbacks import (
    UnlimitedStockDataExtractor,
    ExtractionResult,
    StockData
)
from .data_validation_pipeline import ValidationLevel

load_dotenv()
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

# Global extractor instance for backward compatibility
_global_extractor = None

def _get_global_extractor() -> UnlimitedStockDataExtractor:
    """Get or create global extractor instance"""
    global _global_extractor
    if _global_extractor is None:
        _global_extractor = UnlimitedStockDataExtractor(
            cache_dir=os.getenv('STOCK_CACHE_DIR', '/tmp/unlimited_stock_cache'),
            enable_validation=True,
            validation_level=ValidationLevel.STANDARD,
            enable_caching=True,
            enable_health_monitoring=True,
            max_concurrent=50
        )
        logger.info("Initialized unlimited stock data extractor")
    return _global_extractor

# Backward compatibility classes (deprecated)
class RateLimitConfig:
    """Deprecated: Rate limiting is no longer needed"""
    def __init__(self, *args, **kwargs):
        logger.warning("RateLimitConfig is deprecated - unlimited extraction has no rate limits")
        pass

class DataSourceConfig:
    """Deprecated: Source configuration handled internally"""
    def __init__(self, *args, **kwargs):
        logger.warning("DataSourceConfig is deprecated - sources managed automatically")
        pass

class MultiSourceDataExtractor:
    """
    Backward compatible wrapper for the unlimited extraction system
    Maintains same interface but eliminates rate limits
    """
    
    def __init__(self, cache_dir: str = "/tmp/stock_cache"):
        logger.info("ğŸš€ Initializing UNLIMITED Stock Data Extractor")
        logger.info("âœ“ NO RATE LIMITS - Can handle 6000+ stocks")
        logger.info("âœ“ Multiple free data sources with fallbacks")
        logger.info("âœ“ Intelligent caching and validation")
        
        self.cache_dir = cache_dir
        self.unlimited_extractor = UnlimitedStockDataExtractor(
            cache_dir=cache_dir,
            enable_validation=True,
            validation_level=ValidationLevel.STANDARD,
            enable_caching=True,
            enable_health_monitoring=True,
            max_concurrent=50
        )
        
        # Deprecated attributes for backward compatibility
        self.data_sources = {}  # Empty - sources managed internally
        self.api_keys = {}      # Empty - no API keys needed
        self.call_history = {}  # Empty - no rate limiting
        self.backoff_delays = {} # Empty - no delays needed
        
        logger.info("ğŸ¯ Unlimited extraction system ready!")
    
    async def extract_stock_data(self, ticker: str, **kwargs) -> Dict[str, Any]:
        """
        Extract stock data for a single ticker (backward compatible interface)
        
        Args:
            ticker: Stock symbol to extract
            **kwargs: Ignored for backward compatibility
            
        Returns:
            Dictionary containing stock data or error information
        """
        try:
            result = await self.unlimited_extractor.extract_stock_data(ticker)
            
            if result.success and result.data:
                # Convert StockData to dictionary for backward compatibility
                data_dict = result.data.to_dict()
                data_dict.update({
                    'extraction_success': True,
                    'extraction_source': result.data.source,
                    'extraction_time_ms': result.extraction_time_ms,
                    'cache_hit': result.cache_hit,
                    'data_quality_score': result.data.data_quality_score
                })
                return data_dict
            else:
                return {
                    'ticker': ticker,
                    'extraction_success': False,
                    'error': result.error,
                    'extraction_source': result.source,
                    'extraction_time_ms': result.extraction_time_ms
                }
                
        except Exception as e:
            logger.error(f"Extraction failed for {ticker}: {e}")
            return {
                'ticker': ticker,
                'extraction_success': False,
                'error': str(e),
                'extraction_source': 'unknown',
                'extraction_time_ms': 0
            }
    
    def extract_stock_data_sync(self, ticker: str, **kwargs) -> Dict[str, Any]:
        """Synchronous wrapper for backward compatibility"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.extract_stock_data(ticker, **kwargs))
    
    async def batch_extract(self, tickers: List[str], 
                          batch_size: int = 100,
                          max_concurrent: int = 50,
                          progress_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """
        Extract data for multiple tickers with unlimited capacity
        
        Args:
            tickers: List of stock symbols
            batch_size: Processing batch size
            max_concurrent: Max concurrent extractions
            progress_callback: Optional progress callback
            
        Returns:
            List of extraction results
        """
        logger.info(f"ğŸš€ Starting unlimited batch extraction for {len(tickers)} stocks")
        logger.info("âš¡ NO RATE LIMITS - Processing at maximum speed!")
        
        try:
            results = await self.unlimited_extractor.extract_bulk_data(
                tickers=tickers,
                progress_callback=progress_callback,
                batch_size=batch_size
            )
            
            # Convert ExtractionResults to dictionaries for backward compatibility
            dict_results = []
            for result in results:
                if result.success and result.data:
                    data_dict = result.data.to_dict()
                    data_dict.update({
                        'extraction_success': True,
                        'extraction_source': result.data.source,
                        'extraction_time_ms': result.extraction_time_ms,
                        'cache_hit': result.cache_hit,
                        'data_quality_score': result.data.data_quality_score
                    })
                else:
                    data_dict = {
                        'ticker': result.ticker,
                        'extraction_success': False,
                        'error': result.error,
                        'extraction_source': result.source,
                        'extraction_time_ms': result.extraction_time_ms
                    }
                
                dict_results.append(data_dict)
            
            successful = len([r for r in dict_results if r.get('extraction_success', False)])
            logger.info(f"âœ… Unlimited batch extraction completed: {successful}/{len(tickers)} successful")
            
            return dict_results
            
        except Exception as e:
            logger.error(f"Batch extraction failed: {e}")
            return [
                {
                    'ticker': ticker,
                    'extraction_success': False,
                    'error': str(e),
                    'extraction_source': 'batch_error',
                    'extraction_time_ms': 0
                }
                for ticker in tickers
            ]
    
    def check_rate_limit(self, source: str) -> bool:
        """Deprecated: Always returns True (no rate limits)"""
        logger.debug(f"Rate limit check for {source}: UNLIMITED (always available)")
        return True
    
    def get_extraction_stats(self) -> Dict[str, Any]:
        """Get comprehensive extraction statistics"""
        try:
            stats = self.unlimited_extractor.get_comprehensive_stats()
            
            # Add summary information
            summary = {
                'system_status': 'ğŸš€ UNLIMITED EXTRACTION ACTIVE',
                'rate_limits': 'NONE - Completely unlimited',
                'data_sources': 'Multiple free sources with fallbacks',
                'cache_system': 'Multi-tier intelligent caching',
                'validation': 'Comprehensive data quality validation',
                'capacity': '6000+ stocks without limits'
            }
            
            stats['system_summary'] = summary
            return stats
            
        except Exception as e:
            logger.error(f"Error getting stats: {e}")
            return {'error': str(e)}
    
    def cleanup(self):
        """Cleanup resources"""
        try:
            if hasattr(self, 'unlimited_extractor'):
                self.unlimited_extractor.cleanup()
            logger.info("Cleanup completed")
        except Exception as e:
            logger.error(f"Cleanup error: {e}")

# Primary DataExtractor class for backward compatibility
class DataExtractor(MultiSourceDataExtractor):
    """
    Main data extractor class - now powered by unlimited extraction system
    Maintains complete backward compatibility with existing code
    """
    
    def __init__(self, cache_dir: str = "/tmp/stock_cache"):
        super().__init__(cache_dir)
        
        logger.info("="*60)
        logger.info("ğŸ¯ UNLIMITED STOCK DATA EXTRACTOR INITIALIZED")
        logger.info("="*60)
        logger.info("âœ… NO RATE LIMITS - Handle 6000+ stocks effortlessly")
        logger.info("âœ… Yahoo Finance web scraping (unlimited)")
        logger.info("âœ… SEC EDGAR data (free fundamentals)")
        logger.info("âœ… IEX Cloud free tier")
        logger.info("âœ… Intelligent fallback system")
        logger.info("âœ… Multi-tier caching system")
        logger.info("âœ… Comprehensive data validation")
        logger.info("âœ… Concurrent processing engine")
        logger.info("="*60)

    async def fetch_stock_data(self, ticker: str, **kwargs) -> Dict[str, Any]:
        """Alias for extract_stock_data for compatibility"""
        return await self.extract_stock_data(ticker, **kwargs)
    
    def fetch_stock_data_sync(self, ticker: str, **kwargs) -> Dict[str, Any]:
        """Synchronous fetch for backward compatibility"""
        return self.extract_stock_data_sync(ticker, **kwargs)

# DataValidator class for backward compatibility
class DataValidator:
    """Basic data validation wrapper for backward compatibility"""
    
    def __init__(self):
        logger.info("DataValidator initialized (using comprehensive validation system)")
    
    @staticmethod
    def validate_stock_data(data: Dict) -> bool:
        """
        Validate stock data completeness and quality
        
        Args:
            data: Stock data dictionary
            
        Returns:
            True if data is valid, False otherwise
        """
        try:
            # Basic validation checks
            if not isinstance(data, dict):
                return False
            
            # Check for required fields
            required_fields = ['ticker']
            for field in required_fields:
                if field not in data:
                    return False
            
            # Check extraction success flag
            if 'extraction_success' in data:
                return data['extraction_success']
            
            # Check for reasonable price values
            if 'current_price' in data:
                price = data['current_price']
                if not isinstance(price, (int, float)) or price <= 0:
                    return False
            
            # Check data quality score if available
            if 'data_quality_score' in data:
                return data['data_quality_score'] >= 50
            
            return True
            
        except Exception as e:
            logger.error(f"Validation error: {e}")
            return False

# Factory functions for easy initialization
def create_unlimited_extractor(cache_dir: str = None) -> DataExtractor:
    """
    Create an unlimited data extractor instance
    
    Args:
        cache_dir: Optional cache directory path
        
    Returns:
        Configured DataExtractor instance
    """
    cache_dir = cache_dir or os.getenv('STOCK_CACHE_DIR', '/tmp/unlimited_stock_cache')
    return DataExtractor(cache_dir)

def get_extractor() -> DataExtractor:
    """Get or create global extractor instance"""
    global _global_extractor
    if _global_extractor is None:
        _global_extractor = create_unlimited_extractor()
    return _global_extractor

# Convenience functions for simple usage
async def extract_ticker(ticker: str) -> Dict[str, Any]:
    """
    Quick extraction of single ticker data
    
    Args:
        ticker: Stock symbol
        
    Returns:
        Stock data dictionary
    """
    extractor = get_extractor()
    return await extractor.extract_stock_data(ticker)

def extract_ticker_sync(ticker: str) -> Dict[str, Any]:
    """
    Synchronous version of extract_ticker
    
    Args:
        ticker: Stock symbol
        
    Returns:
        Stock data dictionary
    """
    extractor = get_extractor()
    return extractor.extract_stock_data_sync(ticker)

async def extract_multiple_tickers(tickers: List[str], 
                                 max_concurrent: int = 50) -> List[Dict[str, Any]]:
    """
    Extract data for multiple tickers without rate limits
    
    Args:
        tickers: List of stock symbols
        max_concurrent: Maximum concurrent extractions
        
    Returns:
        List of stock data dictionaries
    """
    extractor = get_extractor()
    return await extractor.batch_extract(
        tickers=tickers,
        max_concurrent=max_concurrent
    )

# Demo and testing functions
async def demo_unlimited_extraction():
    """Demonstrate the unlimited extraction capabilities"""
    print("ğŸš€ UNLIMITED STOCK DATA EXTRACTION DEMO")
    print("="*50)
    
    extractor = create_unlimited_extractor()
    
    try:
        # Demo 1: Single ticker extraction
        print("\nğŸ“ˆ Demo 1: Single Ticker Extraction")
        result = await extractor.extract_stock_data('AAPL')
        
        if result.get('extraction_success'):
            print(f"âœ… AAPL: ${result.get('current_price', 'N/A')} "
                  f"(Source: {result.get('extraction_source', 'unknown')})")
            print(f"   Quality Score: {result.get('data_quality_score', 'N/A')}/100")
            print(f"   Time: {result.get('extraction_time_ms', 'N/A')}ms")
        else:
            print(f"âŒ AAPL extraction failed: {result.get('error', 'Unknown error')}")
        
        # Demo 2: Bulk extraction (no rate limits!)
        print("\nğŸš€ Demo 2: Bulk Extraction (NO RATE LIMITS)")
        test_tickers = ['MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'JNJ', 'WMT']
        
        print(f"Extracting {len(test_tickers)} tickers simultaneously...")
        start_time = datetime.now()
        
        results = await extractor.batch_extract(test_tickers, max_concurrent=50)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        successful = [r for r in results if r.get('extraction_success')]
        
        print(f"âœ… Results: {len(successful)}/{len(test_tickers)} successful")
        print(f"âš¡ Total time: {processing_time:.2f} seconds")
        print(f"ğŸ¯ Rate: {len(test_tickers)/processing_time:.1f} tickers/second")
        print("ğŸ’« NO RATE LIMITS - Could handle 6000+ stocks!")
        
        # Show sample results
        for result in successful[:3]:
            ticker = result.get('ticker', 'Unknown')
            price = result.get('current_price', 'N/A')
            source = result.get('extraction_source', 'unknown')
            print(f"   ğŸ“Š {ticker}: ${price} (source: {source})")
        
        # Demo 3: System statistics
        print("\nğŸ“Š Demo 3: System Statistics")
        stats = extractor.get_extraction_stats()
        
        if 'extraction' in stats:
            extraction_stats = stats['extraction']
            print(f"Total Requests: {extraction_stats.get('total_requests', 0)}")
            print(f"Success Rate: {extraction_stats.get('success_rate', 0)*100:.1f}%")
            
        if 'cache' in stats:
            cache_stats = stats['cache']['overview']
            print(f"Cache Hit Rate: {cache_stats.get('hit_rate', 0)*100:.1f}%")
            
        if 'health' in stats:
            health_stats = stats['health']
            print(f"Healthy Sources: {health_stats.get('healthy_sources', 0)}/{health_stats.get('total_sources', 0)}")
        
        print("\nğŸ‰ Demo completed successfully!")
        print("ğŸš€ Ready to handle 6000+ stocks without any rate limits!")
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        
    finally:
        extractor.cleanup()

def print_migration_guide():
    """Print migration guide for existing users"""
    print("\n" + "="*60)
    print("ğŸ”„ MIGRATION TO UNLIMITED EXTRACTION SYSTEM")
    print("="*60)
    print()
    print("âœ… NO CODE CHANGES REQUIRED!")
    print("   Your existing code will work exactly the same")
    print()
    print("ğŸš€ NEW CAPABILITIES:")
    print("   âœ“ NO RATE LIMITS - Handle 6000+ stocks")
    print("   âœ“ Multiple free data sources")
    print("   âœ“ Intelligent fallback system")
    print("   âœ“ Advanced caching and validation")
    print("   âœ“ Concurrent processing")
    print()
    print("ğŸ“ EXAMPLE USAGE:")
    print("   # Same as before - but now unlimited!")
    print("   extractor = DataExtractor()")
    print("   data = await extractor.extract_stock_data('AAPL')")
    print("   ")
    print("   # Bulk extraction (new capability)")
    print("   tickers = ['AAPL', 'MSFT', 'GOOGL', ...]  # 6000+ tickers!")
    print("   results = await extractor.batch_extract(tickers)")
    print()
    print("ğŸ¯ BENEFITS:")
    print("   âœ“ Faster extraction (no delays)")
    print("   âœ“ Higher reliability (multiple sources)")
    print("   âœ“ Better data quality (validation)")
    print("   âœ“ Automatic caching (faster repeated access)")
    print("   âœ“ Health monitoring (automatic recovery)")
    print()
    print("="*60)

if __name__ == "__main__":
    print_migration_guide()
    print("\nğŸš€ Running demonstration...")
    
    # Run demo
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    asyncio.run(demo_unlimited_extraction())