[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# The executor class that airflow should use. Choices include
# LocalExecutor, CeleryExecutor, KubernetesExecutor, SequentialExecutor
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow_user:${AIRFLOW_DB_PASSWORD}@postgres:5432/airflow_db

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
# in one or across multiple DAGs. This is an upper limit on the number of
# task instances with the state RUNNING or QUEUED.
dag_concurrency = 16

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 1

# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = False

# How long before timing out a python file import
dagbag_import_timeout = 30.0

# How long before timing out a DagFileProcessor, which processes a dag file
dagbag_import_error_tracebacks = True

# The class to use for running task instances in a subprocess.
task_runner = StandardTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
default_impersonation = 

# Secret key to save connection passwords in the db
fernet_key = ${AIRFLOW_FERNET_KEY}

# Whether to disable pickling dags
donot_pickle = True

# How long before timing out a task
task_timeout_minutes = 60

# Whether to store serialized dags in the database
store_serialized_dags = True

# Whether to store DAG code
store_dag_code = True

# Default timezone in case supplied date times are naive
default_timezone = America/New_York

# Catchup by default
catchup_by_default = False

# Default number of retries for a task instance
default_task_retries = 2

# Default retry delay for a task instance (in seconds)
default_task_retry_delay = 300

# The default queue that tasks get assigned to and that workers listen on
default_queue = default

# Number of times the code should be retried in case of DB Operational Errors
max_db_retries = 3

[logging]
# The logging level
logging_level = INFO

# FAB Logging level
fab_logging_level = WARN

# Logging configuration
logging_config_class = airflow.config_templates.airflow_local_settings.DEFAULT_LOGGING_CONFIG

# Whether to use colored console log
colored_console_log = False

# Remote Logging (we'll keep it simple with local for now)
remote_logging = False

[metrics]
# To enable StatsD integration
statsd_on = True
statsd_host = statsd-exporter
statsd_port = 9125
statsd_prefix = airflow

[celery]
# Celery broker URL
broker_url = redis://:${REDIS_PASSWORD}@redis:6379/2

# The Celery result_backend
result_backend = redis://:${REDIS_PASSWORD}@redis:6379/2

# Worker concurrency
worker_concurrency = 4

# Default queue
default_queue = default

# Flower settings
flower_host = 0.0.0.0
flower_url_prefix = 
flower_port = 5555
flower_basic_auth = admin:${FLOWER_PASSWORD}

# SSL options for Celery
ssl_active = False

# Number of seconds to wait for response from a task
operation_timeout = 2.0

# Number of seconds to wait before retrying after a failure
task_ack_late = True

# Pickles messages if True
worker_precheck = False

# Name of the worker
worker_log_server_port = 8793

# Celery pools to use
pool = prefork

# Number of processes when using prefork pool
worker_autoscale = 8,2

# Celery task result expires in
result_expires_in = 3600

# Allow some tasks to run for a while
worker_max_tasks_per_child = 1000

# Worker hijack root logger
worker_hijack_root_logger = True

# Stick to celery < 4.2.0 for now
worker_enable_remote_control = False

[celery_kubernetes_executor]
kubernetes_queue = kubernetes

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# Default timezone to display all dates in the UI, can be UTC, system, or
# any IANA timezone string (e.g. Europe/Amsterdam). If left empty the
# default value of core/default_timezone will be used
default_ui_timezone = America/New_York

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert = 

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key = 

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# If set to True, Airflow will track files in plugins_folder directory. When it detects changes,
# it will restart the gunicorn server
reload_on_plugin_change = False

# Secret key used to run your flask app
secret_key = ${AIRFLOW_SECRET_KEY}

# Number of workers to run the Gunicorn web server
workers = 2

# The worker class gunicorn should use
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = True

# Authentication backend
auth_backend = airflow.contrib.auth.backends.password_auth

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
owner_mode = user

# Default DAG view. Valid values are: tree, graph, duration, gantt, landing_times
dag_default_view = tree

# Default timezone to display all dates in the UI, can be UTC, system, or
# any IANA timezone string (e.g. Europe/Amsterdam). If left empty the
# default value of core/default_timezone will be used
default_ui_timezone = America/New_York

# Automatically logout inactive users (0 for no timeout)
auto_logout_minutes = 60

# Hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# SMTP configuration
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = ${SMTP_USER}
smtp_password = ${SMTP_PASSWORD}
smtp_port = 587
smtp_mail_from = ${SMTP_USER}

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks test`),
# this defines the frequency at which they should listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = True
statsd_host = statsd-exporter
statsd_port = 9125
statsd_prefix = airflow

# How many seconds to wait between file-parsing loops to prevent the logs from being spammed.
min_file_process_interval = 0

# Number of seconds after which a DAG file is parsed. The DAG file is parsed every
# `min_file_process_interval` number of seconds. This can be used to reduce the response
# time when a new DAG file is added.
dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
scheduler_health_check_threshold = 30

# Turn off scheduler catchup by default
catchup_by_default = False

# Number of task instances to be scheduled by the scheduler in one run
max_tis_per_query = 512

# This controls the number of processes that the scheduler will run in parallel
# to parse dags. This should be a small number because it's an expensive operation.
parsing_processes = 2

# Should the scheduler have Gunicorn serve the log files
serve_logs = True

# The maximum number of times to retry a task that failed
max_task_retry_count = 3

# Processor poll interval in seconds
processor_poll_interval = 1

# Scheduler zombie task cleanup interval in seconds
zombie_task_threshold = 300

[api]
# Authentication backend for API
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

# Enable the experimental API 
enable_experimental_api = False

# Maximum allowed page size for API
maximum_page_limit = 100

# Fallback page size if request does not specify a page size
fallback_page_limit = 100

[lineage]
# what lineage backend to use
backend = 

[atlas]
sasl_enabled = False
host = 
port = 21000
username = 
password = 

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue = 

# Credentials to use for Hive connections
hive_cli_conn_id = hive_cli_default

# The command to use to connect to Hive CLI. 
hive_cli_executable = hive

# Comma separated list of hive configuration variables
hive_cli_params = 

# Path to hive CLI executable
hive_metastore_conn_id = hive_metastore_default

[webhdfs]
# The object used to connect to HDFS.
proxy_user = 

[hdfs]
# Snakebite's URL
snakebite_config = 

[kubernetes]
# This section contains Kubernetes related configuration for the KubernetesExecutor
worker_container_repository = 
worker_container_tag = 
namespace = default
worker_pods_creation_batch_size = 1
multi_namespace_mode = False
in_cluster = True
config_file = 
cluster_context = 
git_repo = 
git_branch = 
git_sync_rev = HEAD
git_sync_depth = 1
git_sync_root = /git
git_sync_dest = repo
git_dags_folder_mount_point = 
git_ssh_key_secret_name = 
git_ssh_known_hosts_configmap_name = 
git_sync_credentials_secret = 
git_user = 
git_password = 
delete_worker_pods = True
delete_worker_pods_on_failure = False
worker_pods_pending_timeout = 300

[kubernetes_secrets]
# The kubernetes secret that contains the username for the pods
# AIRFLOW__KUBERNETES_SECRETS__SQL_ALCHEMY_CONN = airflow-secrets=connection
# AIRFLOW__KUBERNETES_SECRETS__FERNET_KEY = airflow-secrets=fernet-key
# AIRFLOW__KUBERNETES_SECRETS__RESULT_BACKEND_CONN = airflow-secrets=result_backend

[kubernetes_environment_variables]
# The kubernetes environment variables that can be found in the pod
# AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__ENV_NAME = configmap-name=config-key

[kubernetes_labels]
# The kubernetes labels to be added to the worker pods
# AIRFLOW__KUBERNETES_LABELS__LABEL_NAME = label_value

[kubernetes_node_selectors]
# The kubernetes node selectors to be applied to the worker pods
# AIRFLOW__KUBERNETES_NODE_SELECTORS__NODE_NAME = node_value

[kubernetes_annotations]
# The kubernetes annotations to be added to the worker pods
# AIRFLOW__KUBERNETES_ANNOTATIONS__ANNOTATION_NAME = annotation_value

# Example pool configurations for investment analysis
[pool_default]
# Default pool for general tasks
default_pool_task_slot_count = 128

# Pool configuration section (this needs to be done via UI or CLI)
# We'll handle this in the init script
# Pool: api_calls (for external API calls - limit based on rate limits)
# Pool: compute_intensive (for ML/analytics tasks)
# Pool: database_tasks (for database operations)
# Pool: low_priority (for maintenance and cleanup tasks)